%&latex
\documentclass[11pt]{asaproc}

\usepackage{graphicx}

%\usepackage{mathtime}

%%UNCOMMENT following line if you have package
\usepackage{times}

\title{Number of Samples Needed For Model Selection With Confidence}

\author{Sean G. Carver\thanks{American University, 4400 Massachusetts
    Avenue NW, Washington DC, 20016}}
\begin{document}
\SweaveOpts{concordance=TRUE}


\maketitle

\begin{abstract}
  A common measure used to quantify the similarity of two models is
  the Kullback-Leibler divergence, computed from a true model to an
  alternative model. We propose a different measure: the number of
  samples needed to correctly reject the alternative model with a
  given confidence level (e.g. 95\%). Our method works as follows: (1)
  we simulate samples from the true model, (2) for each sample, we
  compute a log-likelihood ratio (3), we bootstrap and sum the
  log-likelihood ratios---when this sum is positive, we select the
  true model, (4) using simple linear regression, we determine the
  number of terms (i.e. number of samples) needed to make the desired
  quantile (e.g. 5\%) fall at zero. We have tested this method on
  t-distributions of different degrees of freedom and have confirmed
  that it gives reasonably consistent results. However, we plan to
  apply this method to Markov chains, e.g. used for sports statistics
  like tennis, volleyball, and baseball. For these applications, it
  may be desirable to have a measure that is easier to interpret than
  the Kullback-Leibler divergence. How many innings are needed to
  falsify the model of the Yankees when simulating a model of the
  Orioles?
\begin{keywords}
Model selection, likelihood ratio test, Akaike information criterion,
Kullback-Leibler divergence
\end{keywords}
\end{abstract}

\section{Motivation}
When working with two or more probabilistic models, you may want to
quantify their similarity.  Specifically, when observing simulations
of one model, can you easily tell that the simulations do not come
from another model?  Or do only subtle differences between the models
make this discernment difficult?

My favorite example involves baseball.  What if you had score cards
recording, play after play for many games, what bases had runners, and
how many outs had occurred.  Could you tell which teams were up at
bat?  In practice, there will be many uncontrolled factors, so if you
require a substantial degree of certainty, you will only be able to
distinguish between two mismatched teams.  On the other hand, in
theory, if the teams playing are {\em models of teams}, obeying
precisely defined and known probability laws, and if the models make
different, even slightly different, predictions, you can make the
choice confidently.  Indeed, you can have as high a chance as you
want, short of being absolutely certain, of choosing the right team,
provided you have enough data [Burnham \& Andersen].  But how much
data suffice for the confidence you demand?

Once you derive a model of each team from game records, you can ask
yourself, for example: how similar is the model of the our home team,
the Baltimore Orioles, to the model one of their rivals, the New York
Yankees.  (Baltimore hosted the Joint Statistical Meetings in 2017.)
How many Baltimore-at-bat half-innings would you have to simulate to
correctly reject the statement that the model of the Yankees generated
the simulations, and get this answer right at least 95\% of the time
(or some other specified confidence level)?

\section{Baseball as a Markov Chain}
Baseball is often modeled as a Markov chain [Marchi \& Albert, Albert,
Kemeny \& Snell], where each half-inning proceeds through a number of
states.  The states record which bases have runners, and how many outs
have occurred.  There are 8 possible combinations of runners on base
(labeled: 0, 1, 2, 3, 12, 13, 23, 123), and 4 possible numbers of outs
(labeled: {\em blank}, X, XX, XXX).  Specifically, the state label 0
indicates empty bases with no outs, whereas the state label 123XX
indicates loaded bases with two outs, etc.  All states with three outs
are combined into a single absorbing state: XXX, which signifies the
end of the half-inning.  There exist a total of 25 states and a total
of 600 (equaling $24 \times 25$) conceivable transitions.  We note
that many of these transitions remain impossible, by the rules of
baseball, such as, for example, going from bases empty with two outs,
to bases empty with one out, i.e.\ 0XX:0X.  Using a recursive
algorithm savvy to the rules, I counted 296 allowable transitions
between states, and 304 illegal ones.  Of the 296 allowable
transitions, only 272 occurred at least once in the 2011 Major League
season---24 never occurred.  For example, in 2011 no team underwent
the transition 1X:1X even though it could have occurred, within the
rules of the game.  Had a team undergone this transition, the batter
would have advanced only to first base, while the runner on first
would have scored---clearly an unusual scenario, but not impossible:
both 1:1 and 1XX:1XX did occur in 2011 Major League play.

Since no transitions happen from XXX, we can then define a
$24 \times 25$ matrix of transition probabilities, with entries
$\{p_{i:j}\}$.  Specifically, $p_{i:j}$ is the probability that the
game will transition to state $j$, assuming it finds itself in state
$i$, just prior.  To specify a model of a baseball team, we must
simply specify the 600 transition probabilities.  However, 304 of
these entries must be zero, by the rules.  The rest of the
probabilities will fall between 0 and 1.  Many more of these
probabilities may be zero for a given team.  Additional constraints
stem from the observations that, from each state other than XXX, the
game must, with certainty, go somewhere (but possibly back to the same
state).

To determine the values of these parameters for each team, the
simplest method uses the so-called maximum likelihood estimates
(MLEs) [citation?]:
$$\mbox{MLE of } p_{ij} = \frac{\mbox{number of transitions made from
    $i$ to $j$}}{\mbox{total number of transitions made from $i$}}$$
In calculating the MLEs, use game records for a particular team, if
you want a model of that team.  My team models come only from the game
records of the team batting at home.  I made this choice to make the
models as different as possible.  Even in the Major Leagues, ballparks
differ substantially, and the home field can have a significant effect
on the play.  The park effects add distinctiveness to the models that
also comes from the differences in the team at bat.  Fixing the home
field also reduces variability in the data from game to game.

Unfortunately, baseball models based on MLEs have significant
drawbacks, especially when we consider the models together.  Though,
some transitions are common, many others are not.  Invariably, some of
the more unusual transitions will happen for one team, in one year,
but not for the other.  As a consequence, the MLE model will make some
transitions possible for one team, but not for the other.  For
example, the uncommon transition 23X:3X happened once in 2011 for the
Baltimore Orioles, but never in 2011 for the New York Yankees.  Thus,
if you ever see the 23X:3X transition in a long series of
half-innings, you can immediately reject, with absolute certainty, the
statement that the 2011 MLE model of the Yankees simulated the data.
This automatic rejection makes the correct selection Orioles easy for
a reason that depends more on the noise in the data than on the teams
under study.  Thus, the interpretation of our results will be made in
the context of this rather unfortunate behavior of the MLEs for
baseball.

One possible solution to this problem smooths the transition matrix in
such a way that no transition remains impossible for one team that
stands possible for another.  After all, if a transition truly is
possible, it should not be deemed impossible, even if it never
occurred for a team one year.  There exist principled ways of making
this adjustment, discussed in [Marchi \& Albert, 2013], but they are
beyond the scope of the present paper.

\section{Deciding Between Models}
Our calculations can be made with any models, provided (1) you can
simulate the true model to generate samples, and (2) you can calculate
the likelihood of each of these samples, for both the true model and
for any alternative model under consideration.  The relative values of
these likelihoods determine which model you deem as correct.  Of
course, with these calculations, you know in advance which model is
correct, but knowing this fact allows you to calculate how many
samples you would have needed to be confident, if you did not know.

Each sample from one of our baseball models consists of the game
records of a half-inning of the plays for the team at bat.  Simulating
a half-inning involved starting with the bases empty, no outs (the 0
state) then successively using the transition probabilities to ``throw
the dice'' to see the succession of states, until reaching the end of
the inning, the XXX state.  Each transition between states has a
probability associated with it.  It is well know that in the history
of Major League play, no player has ever consistently batted over
.500, meaning that most plate appearances have led to an out.  From
this fact, we can guess that, for all teams in the Major Leagues,
throughout history, the most common inning is the one where there are
three outs in a row: 0:0X:0XX:XXX.  Indeed, simulations, with the
transition probabilities for the combined 2011 teams, predict
0:0X:0XX:XXX for 31.5\% of the half-innings.  Individual teams will
have a different percentages, depending on the skill of their batters.
True, the most common outcome occurs for considerably less than half
of the half-innings, but the next most common half-inning,
0:X:0XX:1XX:XXX, occurs only 7.6\% of the time.

According to the 2011 data, there are only 9 distinct half-innings
which have probabilities greater than 0.01 (predicted to occur more
than 1\% of the time).  The rest we can call {\em rare} innings.  But
if we sum the probabilities for the non-rare innings we only get
0.555, which means that 44.5\% of the time, we predict that the inning
will be rare.  As a group, rare innings are not particularly rare, but
each one individually is quite rare, occurring than only once in more
than 100 innings.

There are infinitely many possible innings, provided the team at bat
manages to never suffer three outs, and the referee does not call the
game as the score gets increasingly lopsided.  The probabilities of
all possible innings sum exactly to 1 in an infinite convergent
series, so the probabilities of individual innings must drop off
dramatically as the score increases.  But because the numbers get very
small, and very small numbers occur frequently, we work with the logs
of these probabilities, the so-called log-likelihoods.

Baseball is complicated.  For a simpler model, we use the student
t-distribution.  Instead of 600 transition probabilities, its single
parameter is the number of degrees of freedom for the distribution.
Our true model will be a t-distribution with 5 degrees of freedom,
t(5), whereas our alternative model we wish will have an infinite
degrees of freedom, $\mbox{t}(\infty)$, better known as the standard
Normal distribution, N(0,1).

Analogous to a single half-inning in baseball, such as 0:0X:0XX:XXX,
each sample of the t-distribution is a single number on the real line,
either positive, zero, or negative, such as -0.143134.  Once we get
many such samples, the next step is to compute the likelihood of each
sample.  In baseball, the most likely sample is the one just
mentioned, but there were infinitely many others, with lower, sometimes
much lower, likelihoods.  

For baseball, half-innings are discrete.  For the t-distribution,
samples occur on a continuum---in one dimension.  As a consequence,
you can graph the likelihood function, better known as the probability
density function, with the sample on the horizontal axis, and the
likelihood (or probability density) on the vertical axis.  We use
likelihood and probability density interchangeably, but the correct
distinction uses {\em likelihood} when parameter values vary, with
fixed sample, and {\em probability density}, when the sample varies
with fixed parameter values.  Here we vary only parameter values
discretely between the two models (e.g.\ we vary the single number of
degrees of freedom, or the 600 transition probabilities, with
constraints).

For all t-distributions, including N(0,1), the most likely sample is
0.  When likelihood is plotted, 0 will appear at the top of a bell
curve, and away from 0, and the likelihood drops off, with a shape
that depends on the number of degrees of freedom.  The smaller the
number of degrees of freedom, the lower the likelihood in the center,
near 0, and the higher the likelihood in the tails.  Just like the
probabilities of the different innings summed to 1 in an infinite,
convergent series, the probability density integrates to 1 over the
whole range of possible samples (all real numbers).  In other words,
the area under the bell curve is 1, when extended out infinitely far
into the tails.  Just like the probabilities of baseball innings
dropped off as the score increases, the probability densities of
t-samples drops off, as the sample gets farther into the tails, away
from 0.  The key point is that this drop off occurs faster for N(0,1)
than for t(5), just as the drop off for high scores occurs faster for
worse baseball teams, than for better teams.

If you are deciding between t(5) and N(0,1), using one sample, you
make the choice based on which model has a higher likelihood at the
sample.  If the sample is near the center, you pick N(0,1), and if the
sample is in one of the tails you pick t(5).  But, even for t(5), most
samples will fall near the center, so if the true model is t(5), you
get the answer wrong, most of the time.  Only when you take many
samples, and start to fill in a dense histogram of the distribution,
with lots of samples in the tails, are you likely to reject N(0,1), if
that is indeed the correct choice.

How do you make the selection using more than one sample?  All you
need is the likelihoods of each sample (from the true model) for both
the true model and the alternative model.  In baseball, the likelihood
is the computed probability of the simulated half-inning, and for the
t-distributions it is the probability density of the sample.  However
in both cases, we want the log-likelihood.

However, we are interested in the relative likelihoods between the two
models.  We select the model with the greatest likelihood.  This
selection goes by the name {\em likelihood ratio test} and in our
situations, it is equivalent making the decision based on the {\em
  Akaike information criterion} or AIC.  Why are the two equivalent?
Unlike for the likelihood ratio test, the formula for the AIC contains
a term that adjusts for bias that inevitably occurs when the modeler
fits the parameters with the same set of data used to make the
selection.  Our models are fit in advance, and we make our selection
with different data---simulated data---after we fix the parameters.
In our cases, the correction term in the formula for AIC is zero, and
the AIC and the likelihood ratio test, are one and the same.

As suggested by the name {\em likelihood ratio test} we use ratios,
specifically, before taking logs, the quantity of interest is the
likelihood of the true model, over the likelihood of the alternative
model, both at the sample simulated by the true model.  If this
quantity is greater that 1, we choose the true model; otherwise we
choose the alternative model.

When we take logs, the quantity of interest becomes the difference in
log-likelihoods---remember, the log of a ratio is the log of the
numerator minus the log of the denominator.  When this quantity is
greater than zero (i.e.\ $(\log(1)$), we choose the true model;
otherwise, we choose we choose the alternative model.

What if we have more than one sample?  We assume that samples are
independent, so that their probabilites (or probability densities)
multiply.  Now we take the log of the product.  Remember, the log of a
product is the sum of the logs of the factors.  So if, for each
sample, we compute the log-likelihood ratio, we simply need to add the
log-likelihood-ratios, and when this sum is positive, we select the
true model; otherwise we select the alternative model.

The mean of these same log-likelihood ratios is an unbiased
Monte-Carlo estimate of the Kullback-Leibler divergence.  The true
value of the Kullback-Leibler divergence is the expected value of each
log-likelihood ratio.  While Monte-Carlo estimates of the
Kullback-Leibler divergence can be both negative or positive, the true
value is always nonnegative---and positive as long as the models make
different predictions---this result is known as the {\em Gibbs
  inequality}.  It follows from the Gibbs inequality and the central
limit theorem, that we {\em will} select the true model with enough
sample, i.e.\ with a confidence level that approaches 1 as the number
of samples increases, as long as the models make different
predictions.  Baseball models will make different predictions as long
as their transition probability matrices are not exactly the same.

\section{How many samples are needed}

I use a brute force Monte Carlo technique.  Specifically I compute a
matrix of sums of log-likelihood ratios.  The first column contains
single log-likelihood ratios; the second contains sums of two; the
third contains sums of three; and so on.  The entries in the matrix
are of course random, and across rows the entries are derived from
independent samples.  For large enough matrices, these computation
requirements can be prohibitive, so to economize, I compute a smaller
number of samples and their log-likelihood ratios in advance, and use
a bootstrap (random resampling with replacement) to fill in the
matrix.  This procedure introduces a bias, but a bias that is
controllable by setting the number of samples.

Once I compute the matrix, I can compute the proportion (of sums of
log-likelihood ratios) in each column which are positive.  Each
positive entry in the matrix indicates a correct selection of the true
model, and the column number of the matrix indicates the number of
samples used in the selection.  So the column number for which this
proportion crosses the chosen confidence level should be our answer.
Unfortunately, this crossing is not well defined because the
proportion varies randomly, and not in a consistent way across
columns.  As a result, there is a region, which I call the region of
interest where the crossing occurs many times.

My first solution was to zoom in on the region of interest and perform
a linear regression on the proportion to pinpoint an estimate where
the proportion crossed the confidence level.  But the proportion is a
multiple of the number of rows in the matrix and this discretization
is not good for regression.

So instead, I compute quantiles.  For example, for 95\% confidence,
the proportion crosses 0.95, and the same place where the $5^{th}$
percentile crosses 0.  In each case, 95\% of the sums of
log-likelihood ratios are positive.  The quantiles are not discrete,
and regression performs better.

I have tried both simple linear regression (on the quantiles, with the
R command lm()) and quantile regression (directly on the entries in
the matrix, with the R command quantreg::rq(), using the default
choices).  A careful reading of the documentation reveals differences
in between these two procedures, however, there was not a substantial
difference in the quality of the estimates.  That said, computing the
region of interest required the result of an intermediate step (the
quantiles) which was not immediately available in rq().  Therefore I
prefer to use lm().  Having to compute quantiles twice, rq() takes
somewhat longer.

\section{Results}
I calculate that it takes $128 \pm 1$ samples from the t-distribution,
with 5 degrees of freedom, to reject, with 95\% confidence, the
statement that these samples came from the standard normal
distribution.  On the other hand, I calculate that it takes $31 \pm 1$
half-innings simulated from the MLE model derived from the 2011
Baltimore Orioles, batting at home, to reject, with 95\% confidence,
the statment that these half-innings were sampled from the 2011 New
York Yankees MLE model.  One reason why baseball teams are easier to
distinguish than t-distributions, has to do with the above mentioned
properties of the MLE.  Specifically, in a simulation of 10,000 of the
Baltimore-at-bat half-innings, it was discovered that around 2\% had
not occurred for the Yankees while batting in their New York home
stadium in 2011.  If one of these innings occurs in a set of Baltimore
at bat samples, the selection is immediately made for the true model.
At least one such half-inning is expected in approximately 44.5\% of
sequences of 31 half-innings, making the selection problem
substantially easier.

\section{Conclusions}

The number of samples needed for model selection with confidence
offers an alternative to the Kullback-Leibler divergence to quantify
the similarity of two models.  This alternative may be easier to interpret
for those untrained in model selection, and unfamiliar with the
Kullback-Leibler divergence.  However it is discrete measure, unlike
the Kullback-Leibler divergence, and it is substantially harder to
compute.

\section{Acknowledgements}

Rebeca Berger helped prepared the data for processing and adapted the
code from the GitHub companion to the book {\em Analyzing Baseball
  Data with R} [Citation].  Berger's adaptation computes each team's
MLE transition probability matrix from the 2011 data of the team
batting at home.

Our custom code is available at https://github.com/seancarverphd/klir.
The original code is available at
https://github.com/maxtoki/baseball\_R, together with the 2011 data.


% Use BibTeX

\end{document}


